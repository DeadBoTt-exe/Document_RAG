"""
Grounding and safety validation.

Checks whether generated answers are supported by context
and flags hallucinated or unsupported responses.
"""

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI


class GroundingValidator:
    def __init__(self):
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.0,
        )

        self.prompt = PromptTemplate.from_template(
            """
You are a strict factual validator for engineering documentation.

You are given:
- A question
- An answer generated by an AI assistant
- Context retrieved from official documentation

Your task:
Determine whether the answer is COMPLETELY supported by the context.

Rules:
- If every factual claim in the answer is explicitly supported by the context, respond with:
  VALID

- If ANY part of the answer is unsupported, inferred, exaggerated, or missing from the context, respond with:
  INVALID: <short explanation>

Constraints:
- Use ONLY the provided context
- Do NOT rely on prior knowledge
- Do NOT rewrite or improve the answer
- Be strict and conservative

Question:
{question}

Answer:
{answer}

Context:
{context}

Validation result:
"""
        )

        self.chain = self.prompt | self.llm | StrOutputParser()

    def validate(self, *, question: str, answer: str, context: str) -> dict:
        
        result = self.chain.invoke(
            {
                "question": question,
                "answer": answer,
                "context": context,
            }
        ).strip()

        if result == "VALID":
            return {
                "is_valid": True,
                "reason": None,
            }

        return {
            "is_valid": False,
            "reason": result,
        }
